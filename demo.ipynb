{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use vicuna-7b as the target model and vicuna-68m as the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "target_checkpoint = \"lmsys/vicuna-7b-v1.3\"\n",
    "draft_checkpoint = \"double7/vicuna-68m\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_checkpoint)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(target_checkpoint, device_map=device).eval()\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_checkpoint, device_map=device).eval()\n",
    "\n",
    "prompt= \"Long long ago\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to set generation parameters in `model.generation_config` instead of passing them directly into the function `beam_search_by_SD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generation parameters\n",
    "max_new_tokens = 16\n",
    "beam_size = 10\n",
    "draft_beam_size = 10\n",
    "gamma = 2\n",
    "\n",
    "target_model.generation_config.update(**{\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"num_beams\": beam_size,\n",
    "    \"num_return_sequences\": beam_size,\n",
    "    \"return_dict_in_generate\": True,\n",
    "})\n",
    "draft_model.generation_config.update(**{\n",
    "    \"max_new_tokens\": gamma,\n",
    "    \"num_beams\": draft_beam_size,\n",
    "    \"num_return_sequences\": draft_beam_size,\n",
    "    \"return_dict_in_generate\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First generation with resource preparation: 1.71 s\n",
      "['Long long ago, in a land far, far away, there was a beautiful princess named', 'Long long ago, in a land far, far away, there was a small village nestled', 'Long long ago, in a land far, far away, there was a kingdom that was ruled', 'Long long ago, in a galaxy far, far away, there was a time when the', 'Long long ago, in a galaxy far, far away, there was a small planet called', 'Long long ago, in a land far, far away, there was a beautiful princess who', 'Long long ago, in a galaxy far, far away, there was a time when Star', 'Long long ago, in a galaxy far, far away, there was a young man named', 'Long long ago, in a galaxy far far away, there was a planet called Earth.', 'Long long ago, in a galaxy far, far away, there was a planet called Earth']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd4timing import Timer\n",
    "with Timer() as timer_first:\n",
    "    outputs = target_model.generate(**inputs)\n",
    "print(f\"First generation with resource preparation: {timer_first.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers (beam search by batch): 1.15 s\n",
      "['Long long ago, in a land far, far away, there was a beautiful princess named', 'Long long ago, in a land far, far away, there was a small village nestled', 'Long long ago, in a land far, far away, there was a kingdom that was ruled', 'Long long ago, in a galaxy far, far away, there was a time when the', 'Long long ago, in a galaxy far, far away, there was a small planet called', 'Long long ago, in a land far, far away, there was a beautiful princess who', 'Long long ago, in a galaxy far, far away, there was a time when Star', 'Long long ago, in a galaxy far, far away, there was a young man named', 'Long long ago, in a galaxy far far away, there was a planet called Earth.', 'Long long ago, in a galaxy far, far away, there was a planet called Earth']\n"
     ]
    }
   ],
   "source": [
    "with Timer() as timer_TF:\n",
    "    outputs = target_model.generate(**inputs)\n",
    "print(f\"transformers (beam search by batch): {timer_TF.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atspeed (beam search by tree attention): 1.06 s\n",
      "['Long long ago, in a land far, far away, there was a beautiful princess named', 'Long long ago, in a land far, far away, there was a small village nestled', 'Long long ago, in a land far, far away, there was a kingdom that was ruled', 'Long long ago, in a galaxy far, far away, there was a time when the', 'Long long ago, in a galaxy far, far away, there was a small planet called', 'Long long ago, in a land far, far away, there was a beautiful princess who', 'Long long ago, in a galaxy far, far away, there was a time when Star', 'Long long ago, in a galaxy far, far away, there was a young man named', 'Long long ago, in a galaxy far far away, there was a planet called Earth.', 'Long long ago, in a galaxy far, far away, there was a planet called Earth']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd_replace import replace_beam_search_with_TreeAttn\n",
    "\n",
    "model = target_model\n",
    "replace_beam_search_with_TreeAttn(model)\n",
    "with Timer() as timer_TreeAttn:\n",
    "    outputs = model.generate(**inputs)\n",
    "print(f\"atspeed (beam search by tree attention): {timer_TreeAttn.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atspeed (beam search by Speculative Decoding): 1.27 s, accepted_steps: 0\n",
      "target_time_cost: 1.06 s, draft_time_cost: 0.10 s, verify_time_cost: 0.03 s\n",
      "['Long long ago, in a land far, far away, there was a beautiful princess named', 'Long long ago, in a land far, far away, there was a small village nestled', 'Long long ago, in a land far, far away, there was a kingdom that was ruled', 'Long long ago, in a galaxy far, far away, there was a time when the', 'Long long ago, in a galaxy far, far away, there was a small planet called', 'Long long ago, in a land far, far away, there was a beautiful princess who', 'Long long ago, in a galaxy far, far away, there was a time when Star', 'Long long ago, in a galaxy far, far away, there was a young man named', 'Long long ago, in a galaxy far far away, there was a planet called Earth.', 'Long long ago, in a galaxy far, far away, there was a planet called Earth']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd4timing import beam_search_by_SD_4timing\n",
    "\n",
    "outputs = beam_search_by_SD_4timing(target_model, draft_model, inputs)\n",
    "print(f\"atspeed (beam search by Speculative Decoding): {outputs['time_cost']:.2f} s, accepted_steps: {outputs['total_accept_steps']}\")\n",
    "print(f\"target_time_cost: {outputs['target_time_cost']:.2f} s, draft_time_cost: {outputs['draft_time_cost']:.2f} s, verify_time_cost: {outputs['verify_time_cost']:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"beam_sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when `gamma` and `max_new_tokens` are small, the time cost of target model forward in _atspeed (beam search by Speculative Decoding)_ is almost the same as in _atspeed (beam search by tree attention)_. This means that even if the accepted steps = 0, there can still be potential acceleration effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tallrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
