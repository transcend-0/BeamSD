{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage 1\n",
    "\n",
    "Replace transformers' beam search with atspeed's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "target_checkpoint = \"/storage/syma/models/vicuna-7b-v1.3/\"\n",
    "draft_checkpoint = \"/storage/syma/models/vicuna-68m/\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_checkpoint)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(target_checkpoint, device_map=device).eval()\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_checkpoint, device_map=device).eval()\n",
    "\n",
    "prompt= \"Long long ago\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generation parameters\n",
    "max_new_tokens = 10\n",
    "beam_size = 10\n",
    "draft_beam_size = 10\n",
    "gamma = 3\n",
    "\n",
    "target_model.generation_config.update(**{\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"num_beams\": beam_size,\n",
    "    \"num_return_sequences\": beam_size,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "})\n",
    "draft_model.generation_config.update(**{\n",
    "    \"max_new_tokens\": gamma,\n",
    "    \"num_beams\": draft_beam_size,\n",
    "    \"num_return_sequences\": draft_beam_size,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First generation with resource preparation: 2.14 s\n",
      "['Long long ago, in a galaxy far, far away,', 'Long long ago, in a galaxy far, far away...', 'Long long ago, in a land far, far away, there', 'Long long ago, in a galaxy far, far away…', 'Long long ago, in a galaxy far far away, there', 'Long long ago, in a land far far away, there was', 'Long long ago, in a faraway land, there was a', 'Long long ago, in a far-off land, there was', 'Long long ago, in a far far away land, there was', 'Long long ago, in a galaxy far far away...\\n']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd4timing import Timer\n",
    "with Timer() as timer_first:\n",
    "    outputs = target_model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=beam_size)\n",
    "print(f\"First generation with resource preparation: {timer_first.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers (beam search by batch): 0.89 s\n",
      "['Long long ago, in a galaxy far, far away,', 'Long long ago, in a galaxy far, far away...', 'Long long ago, in a land far, far away, there', 'Long long ago, in a galaxy far, far away…', 'Long long ago, in a galaxy far far away, there', 'Long long ago, in a land far far away, there was', 'Long long ago, in a faraway land, there was a', 'Long long ago, in a far-off land, there was', 'Long long ago, in a far far away land, there was', 'Long long ago, in a galaxy far far away...\\n']\n"
     ]
    }
   ],
   "source": [
    "with Timer() as timer_TF:\n",
    "    outputs = target_model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=beam_size)\n",
    "print(f\"transformers (beam search by batch): {timer_TF.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atspeed (beam search by TreeAttn): 0.75 s\n",
      "['Long long ago, in a galaxy far, far away,', 'Long long ago, in a galaxy far, far away...', 'Long long ago, in a land far, far away, there', 'Long long ago, in a galaxy far, far away…', 'Long long ago, in a galaxy far far away, there', 'Long long ago, in a land far far away, there was', 'Long long ago, in a faraway land, there was a', 'Long long ago, in a far-off land, there was', 'Long long ago, in a far far away land, there was', 'Long long ago, in a galaxy far far away...\\n']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd_replace import replace_beam_search_with_speculative_decoding\n",
    "\n",
    "model = target_model\n",
    "replace_beam_search_with_speculative_decoding(model)\n",
    "with Timer() as timer_AT:\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=beam_size)\n",
    "print(f\"atspeed (beam search by TreeAttn): {timer_AT.time_cost:.2f} s\")\n",
    "print(tokenizer.batch_decode(outputs[\"sequences\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atspeed-draft: 0.91 s, accepted_steps: 0\n",
      "target_time_cost: 0.72 s, draft_time_cost: 0.10 s, verify_time_cost: 0.02 s\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd4timing import BSSD4timming\n",
    "\n",
    "outputs = BSSD4timming(target_model, draft_model, inputs, gamma, max_new_tokens)\n",
    "print(f\"atspeed-draft: {outputs['time_cost']:.2f} s, accepted_steps: {outputs['total_accept_steps']}\")\n",
    "print(f\"target_time_cost: {outputs['target_time_cost']:.2f} s, draft_time_cost: {outputs['draft_time_cost']:.2f} s, verify_time_cost: {outputs['verify_time_cost']:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atspeed-draft: 0.59 s, accepted_steps: 0\n",
      "['Long long ago.\\nI\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\'m not sure what you mean by \"', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well. I hope', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\n\\n', 'Long long ago!\\nI hope you have a great day!\\n\\nI hope you have a great day too!\\n\\nI hope you have a great day too!', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nI', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well.\\nI hope you are doing well. I', \"Long long ago.\\nI don't know if it's just me or if it's just me or if it's just me or if it's\", 'Long long ago!\\nI hope you have a great day!\\n\\nBest regards,\\n[Your Name]\\n[Your Email]\\n[Your Phone Number]\\n', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well.\\n\\nI hope you are doing well.', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well. I wish', 'Long long ago!\\nI hope you have a great day!\\n\\nI hope you have a great day too!\\n\\nI hope you have a great day!\\n', 'Long long ago.\\nI\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\'m not sure what you meant by \"', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nPlease', 'Long long ago.\\nI\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\'m not sure what you mean with \"', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nAnd', 'Long long ago!\\nI hope you have a great day!\\n\\nBest regards,\\n[Your Name]\\n[Your Email]\\n[Your Phone]\\n[', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nBut', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nYou', 'Long long ago.\\nI hope you are doing well. I hope you are doing well. I hope you are doing well. I hope you are doing well.\\nIt', 'Long long ago.\\nI\\'m not sure what you mean by \"I\\'m not sure what you mean by \"but I\\'ll do my best to help you']\n"
     ]
    }
   ],
   "source": [
    "from atspeed.beamsd import beam_search_by_speculative_decoding\n",
    "\n",
    "with Timer() as timer_ATSD:\n",
    "    outputs = beam_search_by_speculative_decoding(target_model, draft_model, inputs, gamma, max_new_tokens)\n",
    "print(f\"atspeed-draft: {timer_ATSD.time_cost:.2f} s, accepted_steps: {outputs['total_accept_steps']}\")\n",
    "print(tokenizer.batch_decode(outputs[\"beam_sequence\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage 2\n",
    "\n",
    "Use atspeed's beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tallrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
